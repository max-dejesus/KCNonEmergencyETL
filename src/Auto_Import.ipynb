{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWjKBkChbl_-"
      },
      "source": [
        "## This notebook can be used for the automated funcitonality to auto-populate the clean collection when an update is detected from the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugN2zcurHnMy"
      },
      "outputs": [],
      "source": [
        "from sodapy import Socrata\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType\n",
        "from pymongo import MongoClient, UpdateOne\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import io\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source - https://stackoverflow.com/a\n",
        "# Posted by Till Hoffmann\n",
        "# Retrieved 2025-11-21, License - CC BY-SA 4.0\n",
        "\n",
        "# Allows for logging to return terminal functionality at runtime.\n",
        "def console_handler(stream='stdout'):\n",
        "    \"\"\"\n",
        "    Create a handler for logging to the original console.\n",
        "    \"\"\"\n",
        "    assert stream in {'stdout', 'stderr'}, \"stream must be one of 'stdin' or 'stdout'\"\n",
        "    # Get the file handle of the original std stream.\n",
        "    fh = getattr(sys, stream)._original_stdstream_copy\n",
        "    # Create a writable IO stream.\n",
        "    stream = io.TextIOWrapper(io.FileIO(fh, 'w'))\n",
        "    # Set up a stream handler.\n",
        "    return logging.StreamHandler(stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    stream=sys.stderr,   # send to console directly\n",
        "    level=logging.INFO,  # or DEBUG, WARNING, etc.\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "logger.addHandler(console_handler())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flag for notebook early termination. If False, cells will not execute.\n",
        "CONTINUE_FLAG = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuPtO0cwKslg"
      },
      "outputs": [],
      "source": [
        "DATAKC_APP_TOKEN = os.environ.get('DATAKC_APP_TOKEN')\n",
        "CONN_STR = os.environ.get('MONGO_CONN_STRING')\n",
        "\n",
        "client = Socrata('data.kcmo.org', DATAKC_APP_TOKEN, timeout=60)\n",
        "conn = CONN_STR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BospnCgaK2CE"
      },
      "outputs": [],
      "source": [
        "if CONTINUE_FLAG:\n",
        "  try:\n",
        "    last_mod = client.get_metadata(\"d4px-6rwg\")['viewLastModified']\n",
        "  except Exception as e:\n",
        "    raise Exception(\"Unable to connect to DataKC API. \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PYT8FaQIfb_"
      },
      "outputs": [],
      "source": [
        "if CONTINUE_FLAG:\n",
        "  try:\n",
        "    mongo = MongoClient(conn)\n",
        "    db = mongo.get_database(\"kc_311_db\")\n",
        "    md = db.get_collection(\"ingest_metadata\")\n",
        "\n",
        "    # Find the most recent document from metadata collection\n",
        "    latest_md = md.find().sort([('notebook_timestamp', -1)]).limit(1).next()\n",
        "  except Exception as e:\n",
        "    raise Exception(\"Unable to connect to Mongo successfully. \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONTINUE_FLAG:\n",
        "    if not latest_md['api_last_modified'] < last_mod:\n",
        "        # Exit, no new data to post.\n",
        "        logger.info(\"INFO - No update available from DataKC. Exiting.\")\n",
        "        mongo.close()\n",
        "        CONTINUE_FLAG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0RyLudVL-9T"
      },
      "outputs": [],
      "source": [
        "# If you've made it here, congrats! There's mew data to pull.\n",
        "if CONTINUE_FLAG:\n",
        "        logger.info(\"INFO - New data available, continuing import.\")\n",
        "        spark = SparkSession.builder \\\n",
        "                .master('local[1]') \\\n",
        "                .appName('testapp') \\\n",
        "                .getOrCreate()\n",
        "        spark.sparkContext.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1F01WXWiBK6"
      },
      "outputs": [],
      "source": [
        "if CONTINUE_FLAG:\n",
        "    last_ingest = datetime.fromtimestamp(latest_md['api_last_modified']).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # This will give list of dicts from the API.\n",
        "    # Aim to limit by ~10k records and then hit the endpoint as many times as needed for the update (using the offest).\n",
        "    results_json = client.get(dataset_identifier=\"d4px-6rwg\",\n",
        "                            select='',\n",
        "                            limit=10_000,\n",
        "                            offset=0,\n",
        "                            where=f'''\n",
        "                            date_trunc_ymd(last_updated) >= \"{last_ingest}\"\n",
        "                            AND 1=1\n",
        "                            ''',\n",
        "                            order='last_updated desc'\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load in the expected schema\n",
        "if CONTINUE_FLAG:\n",
        "    schema_json = json.loads('{\"fields\":[{\"metadata\":{},\"name\":\"additional_questions\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"council_district\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"current_status\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"days_to_close\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"department_work_group\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"incident_address\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"issue_sub_type\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"issue_type\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"last_updated\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lat_long\",\"nullable\":true,\"type\":{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}},{\"metadata\":{},\"name\":\"latitude\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"longitude\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"open_date_time\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"report_source\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"reported_issue\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"resolved_date\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"source_category\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"workorder_\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}')\n",
        "    schema = StructType.fromJson(schema_json)\n",
        "    data = spark.createDataFrame(results_json, schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7QAECMRNrLZ"
      },
      "source": [
        "Process and transform begins here, anything before this should handle the case where no updates are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ABH9MnHQORC"
      },
      "outputs": [],
      "source": [
        "if CONTINUE_FLAG:\n",
        "    # Drop unnecesary columns\n",
        "    data = data.drop('workorder_')\n",
        "    data = data.drop('incident_address')\n",
        "    data = data.drop('lat_long')\n",
        "    data = data.drop('additional_questions')\n",
        "\n",
        "    # Change col names for clarity\n",
        "    data = data.withColumn('issue_id', data['reported_issue'])\n",
        "    data = data.drop('reported_issue')\n",
        "\n",
        "    # Parse out dates and times for timestamp col's (more usable w/ visualization)\n",
        "    data = data.withColumn('last_updated_ymd', F.date_format('last_updated', 'yyyy-MM-dd'))\n",
        "    data = data.withColumn('last_updated_hms', F.date_format('last_updated', 'hh:mm:ss'))\n",
        "    data = data.drop('last_updated')\n",
        "\n",
        "    data = data.withColumn('resolved_date_ymd', F.date_format('resolved_date', 'yyyy-MM-dd'))\n",
        "    data = data.withColumn('resolved_date_hms', F.date_format('resolved_date', 'hh:mm:ss'))\n",
        "    data = data.fillna(float('nan'), subset=[\"resolved_date_ymd\", 'resolved_date_hms'])\n",
        "    data = data.drop('resolved_date')\n",
        "\n",
        "    data = data.withColumn('open_date_time_ymd', F.date_format('open_date_time', 'yyyy-MM-dd'))\n",
        "    data = data.withColumn('open_date_time_hms', F.date_format('open_date_time', 'hh:mm:ss'))\n",
        "    data = data.drop('open_date_time')\n",
        "\n",
        "    # Lower any cols w/ different cases\n",
        "    data = data.withColumn('current_status', F.lower('current_status'))\n",
        "\n",
        "    # Add timestamp to new records\n",
        "    data = data.withColumn('ingest_timestamp', F.unix_timestamp(F.current_timestamp()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANGJ-1qwQDdS"
      },
      "outputs": [],
      "source": [
        "# Get count of new rows\n",
        "if CONTINUE_FLAG:\n",
        "    added_rows = data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3wWzoPFSb9v"
      },
      "outputs": [],
      "source": [
        "# Post new clean data\n",
        "if CONTINUE_FLAG:\n",
        "  dash_collection = db.get_collection('dashboard_data')\n",
        "  operations = [\n",
        "      UpdateOne({\"issue_id\": row[\"issue_id\"]}, {\"$set\": row}, upsert=True)\n",
        "      for row in (r.asDict() for r in data.toLocalIterator())\n",
        "  ]\n",
        "  \n",
        "  result = dash_collection.bulk_write(operations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjgo0_H7SU5O",
        "outputId": "02925125-9da7-4d3f-a135-a262a70a2d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "InsertOneResult(ObjectId('691ff059ec21ebef69be9346'), acknowledged=True)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Post metadata after completion\n",
        "if CONTINUE_FLAG:\n",
        "    doc = {\n",
        "        \"api_last_modified\" : last_mod,\n",
        "        \"notebook_timestamp\" : datetime.now().timestamp(),\n",
        "        \"inserted_records\" : added_rows,\n",
        "    }\n",
        "\n",
        "    md.insert_one(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNxi6FzIYI9X"
      },
      "outputs": [],
      "source": [
        "# Cleanup and reporting\n",
        "if CONTINUE_FLAG:\n",
        "    logger.info(f'INFO - Import successfully completed. {added_rows} rows from the API, {result.bulk_api_result['nUpserted']} new and {result.bulk_api_result['nModified']} changed to \\\"{dash_collection.name}\\\".')\n",
        "    mongo.close()\n",
        "    spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
